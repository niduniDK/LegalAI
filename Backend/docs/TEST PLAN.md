# Legal AI

# Master Test Plan

Version 1.0

## Revision History

| Date       | Version              | Description         | Author       |
| ---------- | -------------------- | ------------------- | ------------ |
| 03/10/2025 | 1.0                  | Initial Test Report | Kasige N. D. |
| 05/10/2025 | 1.1 ## 6. References |

- **Testing Frameworks**: pytest (https://pytest.org/), Jest (https://jestjs.io/), Cypress (https://cypress.io/) - Accessed October 2025
- **Load Testing Tools**: Locust (https://locust.io/), JMeter (https://jmeter.apache.org/) - Accessed October 2025
- **UI Testing**: Selenium WebDriver (https://www.selenium.dev/), Playwright (https://playwright.dev/) - Accessed October 2025
- **Performance Monitoring**: New Relic (https://newrelic.com/), Prometheus (https://prometheus.io/) - Accessed October 2025
- **Database Testing**: Supabase CLI (https://supabase.com/docs/guides/cli), pgAdmin (https://www.pgadmin.org/) - Accessed October 2025
- **AI Testing**: Google AI Studio (https://makersuite.google.com/app/apikey) for API testing - Accessed October 2025
- **Code Coverage**: pytest-cov (https://pytest-cov.readthedocs.io/), Istanbul (https://istanbul.js.org/) - Accessed October 2025

**Standards and Methodologies:**

- IEEE Standard for Software Test Documentation (IEEE 829-2008)
- ISTQB Certified Tester Foundation Level syllabus
- OWASP Testing Guide for security testing principles

**Project Documentation:**

- LegalAI Software Requirements Specification (docs/Software Requirements Specification.pdf)
- LegalAI Software Architecture Document (docs/Software Architecture Document.pdf)
- LegalAI Feasibility Report (docs/Feasibility Report.pdf)ed with project specifics, filled placeholders, added comprehensive testing details | GitHub Copilot |

## Table of Contents

- **Load Testing Tools**: Locust (https://locust.io/), JMeter (https://jmeter.apache.org/) - Accessed October 2025
- **UI Testing**: Selenium WebDriver (https://www.selenium.dev/), Playwright (https://playwright.dev/) - Accessed October 2025
- **Performance Monitoring**: New Relic (https://newrelic.com/), Prometheus (https://prometheus.io/) - Accessed October 2025
- **Database Testing**: Supabase CLI (https://supabase.com/docs/guides/cli), pgAdmin (https://www.pgadmin.org/) - Accessed October 2025
- **AI Testing**: Google AI Studio (https://makersuite.google.com/app/apikey) for API testing - Accessed October 2025
- **Code Coverage**: pytest-cov (https://pytest-cov.readthedocs.io/), Istanbul (https://istanbul.js.org/) - Accessed October 2025

**Standards and Methodologies:**

- IEEE Standard for Software Test Documentation (IEEE 829-2008)
- ISTQB Certified Tester Foundation Level syllabus
- OWASP Testing Guide for security testing principles

**Project Documentation:**

- LegalAI Software Requirements Specification (docs/Software Requirements Specification.pdf)
- LegalAI Software Architecture Document (docs/Software Architecture Document.pdf)
- LegalAI Feasibility Report (docs/Feasibility Report.pdf)mits exceeded during testing. | Implement request throttling, caching, and mock responses for AI services during load testing. | Use mock AI responses or reduce test frequency to stay within API limits. |
  | Legal document processing failures (OCR, text extraction). | Test with various document formats and include fallback mechanisms for processing failures. | Skip problematic documents or implement manual review processes. |
  | Third-party service outages (Supabase, Google Gemini). | Design system with offline capabilities and implement retry mechanisms with exponential backoff. | Switch to backup services or implement degraded functionality mode. |
  | Data privacy compliance issues with legal information. | Implement data anonymization, audit logging, and comply with Sri Lankan data protection regulations. | Remove sensitive test data and implement additional privacy controls. |
  | Multilingual content testing complexity (English/Sinhala). | Develop comprehensive test cases for both languages and cultural context validation. | Focus testing on primary language (English) first, then expand to Sinhala. |

## 6. References Database requires refresh. | Implement automated database seeding and refresh scripts for consistent test environments. | Restore from backup and re-run failed tests. | |

| | | | |
| | | | |
| | | | |

## Table of Contents

1. Evaluation Mission and Test Motivation
2. Target Test Items
3. Test Approach  
   3.1 Testing Techniques and Types  
    3.1.1 Data and Database Integrity Testing  
    3.1.2 Function Testing  
    3.1.3 User Interface Testing  
    3.1.4 Performance Profiling  
    3.1.5 Load Testing  
    3.1.6 Security and Access Control Testing  
    3.1.7 Failover and Recovery Testing  
    3.1.8 Configuration Testing
4. Deliverables  
   4.1 Test Evaluation Summaries  
   4.2 Reporting on Test Coverage
5. Risks, Dependencies, Assumptions, and Constraints
6. References

# Master Test Plan

## 1. Evaluation Mission and Test Motivation

The mission and motivation for this testing iteration of Legal AI, Simple Legal Information Retrieval System for Sri Lanka, system is to ensure that the developed system works without bugs and errors. Legal information is highly sensitive and needs to be more accurate when providing to users. Being a legal information retrieval system, Legal AI should be an accurate information provider on Sri Lankan legal context. And it should meet the basic user requirements such as relevant document retrieval for a given user query, answering natural language questions in legal context, accessing previously viewed documents and previous chats on legal context. Assessing to what extent the system meets the user and developer requirements is the key motivation of this testing iteration.

Accessing legal information for a particular situation, is a decent problem for most of the people in Sri Lanka, due to poor literacy on legal domain. Furthermore, there isn't any platform which provide the facility to get an understanding (in simple language) about different acts and laws enacted in Sri Lanka. Legal AI provides a practical and reliable solution for the above problem. There are core features in the system such as chatbot feature, document search feature, document summary retrieval, and access to previously accessed discussions and documents. In addition to the above features, we support providing document recommendations based on previous user searches and let the users to access the legal documents in their preferred language (Sinhala or English).

The architecture of the developed solution consists of web-based system having hybrid retrieval (keyword search and vector search), which is the core of it. All the other functionalities are based on the retriever module. The change is on the representation of the data being retrieved. Also, there is a summary viewing option for the documents being retrieved in the system.

The history of the project involves text extraction from the legal documents (which involves ocr extraction for some documents), cleaning the extracted texts to remove common patterns, chunking the texts, encoding the texts using embedding model(legal-bert) and building the bm25 corpus for hybrid retrieval. Additionally, it involves backend services for processing natural language queries, retrieve respective summaries, retrieve documents for a give query, get recommended documents based on the user search history and backend services for database access. So far, we have focused on developing the features to satisfy the user requirements.

In this test iteration, the mission includes the following concerns,

- Verify the core user requirements

One of the key concerns of this test iteration is to check whether the system satisfies the user requirements.

- 1. Identify the functional requirements.

  - Identify the non-functional requirements

- Find and fix the bugs

To ensure the system performs as expected, we test the system to find any errors and bugs,

- Assess perceived quality risks
- Advise about product

## 2. Target Test Items

The target test items of the system, Legal AI includes both software components and third-party APIs.

- Software Component
  - User Interface (Next.js React frontend)
  - Retriever Module (FAISS vector search + BM25 keyword search)
  - Query Processor (AI-powered query understanding)
  - Summary Generation (Document summarization service)
  - Chatbot Functionality (Google Gemini AI integration)
  - User Authentication (JWT-based auth)
  - Database System (Supabase PostgreSQL)
  - Document Recommendation System (Personalized recommendations)
  - API Routes (FastAPI endpoints)
- Third-party services and APIs
  - Supabase Authentication and Database
  - FastAPI web framework
  - NextAuth.js authentication
  - Google Gemini AI API
  - SMTP Email services (Gmail)
- Web Browser Support (Chrome, Firefox, Safari, Edge)
- Data Processing Pipeline (Jupyter notebooks for document processing)

## 3. Test Approach

The test approach we'll follow both manual and automated testing approach. We performed testing to verify and validate the performance and validate the requirements. For that we mainly perform testing for Chatbot component, user authentication system, backend services for query processor, document retrieval, text summary generation, database integration and recommended document retrieval. Furthermore, we perform user interface testing for the web application to verify that the system provides a decent user experience with our system.

- Chatbot Component
- Text Summary Generation
- Backend Services
  - Query Processor
  - Document Retrieval
  - Document Analysis with Text Summary
  - Document Search Engine
  - Recommendation Pipeline
  - Get Recent User Activity
- User Authentication
- User Interface

One aspect to consider for the test approach is the techniques to be used. This should include an outline of how each technique can be implemented, both from a manual and/or an automated perspective, and the criterion for knowing that the technique is useful and successful. For each technique, provide a description of the technique and define why it is an important part of the test approach by briefly outlining how it helps achieve the Evaluation Mission or addresses the Test Motivators.

Another aspect to discuss in this section is the Fault or Failure models that are applicable and ways to approach evaluating them.

As you define each aspect of the approach, you should update Section Test Environment Configuration, to document the test environment configuration and other resources that will be needed to implement each aspect.]

### 3.1 Testing Techniques and Types

## 3.1.1 Data and Database Integrity Testing

The Supabase PostgreSQL database and data processes should be tested as an independent subsystem. This testing should verify data persistence, user authentication data, chat histories, document metadata, and recommendation data integrity.

| Technique Objective    | Exercise database access methods and processes independent of the UI to observe and log incorrect functioning, data corruption, or integrity violations in the Supabase PostgreSQL database.                                                                                                                                                                                                                                                               |
| ---------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Technique              | · Invoke each database access method (user auth, chat storage, document indexing) using direct SQL queries and API calls.<br>· Seed database with valid and invalid data including legal document metadata, user credentials, and chat histories.<br>· Inspect database tables to ensure data is populated correctly and all database triggers/events execute properly.<br>· Test data relationships between users, chats, documents, and recommendations. |
| Oracles                | Automated scripts to verify data consistency, foreign key relationships, and data type validation. Manual inspection of database logs for error conditions. Comparison of input data against stored/retrieved data.                                                                                                                                                                                                                                        |
| Required Tools         | - Supabase CLI and dashboard for direct database access<br>- PostgreSQL client tools (psql, pgAdmin)<br>- Database testing frameworks (pytest with database fixtures)<br>- Data generation tools for creating test legal documents and user data<br>- Database backup/restore utilities                                                                                                                                                                    |
| Success Criteria       | All database operations complete without errors, data integrity is maintained across CRUD operations, and relationships between legal documents, users, and chat sessions remain consistent.                                                                                                                                                                                                                                                               |
| Special Considerations | - Test with both Supabase's managed PostgreSQL and potential migration scenarios<br>- Include legal document metadata validation (PDF checksums, text extraction integrity)<br>- Verify GDPR compliance for user data handling<br>- Test database performance with realistic legal document volumes (thousands of acts, bills, and gazettes)   

#### 3.1.2 Function Testing

The functional testing of the system Legal AI mainly covers the validation of core functions of user authentication and other backend services. For this we follow the **white box testing technique**, which assesses the internal functions and core business logic and all the helper functions being used. For this we'll write test functions using the python library called **Pytest**.

| Technique Objective    | - Validation of the business logic: This ensures that the functions related to core business logic and internal functions work properly as expected. This includes testing core functions such as process_query(), faiss_retrieve(), etc and the helper functions such as get_pdfs().<br>- Verification of the system requirements: This verifies that the system meets requirements specified by the client and the developer.<br>- Input and Output Validation: This validates the system handles the input and output properly. |
| ---------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Technique              | Execute a separate test for each function such as query processing, document retrieval etc. using Pytest and record the test logs to identify the bugs and errors.<br>Use python assert function to perform input output type validation and input valid and invalid data. (For example, queries relevant to legal domain and irrelevant to legal domain.)                                                                                                                                                                         |
| Oracles                | - The system should not give any response to queries that are irrelevant to legal domain.<br>- The system should give reasonable response for natural language queries related to legal domain.<br>- System should retrieve relevant documents for a given user query.<br>- Documents suggested by the system should match the previous user searches.                                                                                                                                                                             |
| Required Tools         | - Test scripts for backend services were written using Pytest library.<br>- GitHub Copilot to generate test data for valid and invalid user queries.                                                                                                                                                                                                                                                                                                                                                                               |
| Success Criteria       | - All functions related to the business logic of the system should work as expected. (Without being fail.)<br>- All key features such as document search, chatbot response generation with natural language query processing, document recommendation, accessing user search and chat history and user authentication should work.<br>- Proper error handling techniques are applied to make the system susceptible to any condition.                                                                                              |
| Special Considerations | The accuracy of the responses generated by the chatbot relies on the Gemini LLM.                                                                                                                                                                                                                                                                                                                                                                                                                                                   |

#### 3.1.3 User Interface Testing

User Interface testing for Legal AI verifies that the Next.js 15 React frontend provides intuitive access to legal documents and enables seamless interaction with the chatbot, search, recommendations, and history features. Testing focuses on the responsive design across desktop and mobile devices, multilingual support (English/Sinhala), and accessibility compliance. For this we use **Playwright** and **Cypress** for automated end-to-end testing.

| Technique Objective    | - Verify accessibility and functionality of all UI components: Ensures that ChatInterface, DiscoverInterface, RecommendationInterface, ChatSessionsInterface, and HistroyInterface work correctly.<br>- Validate responsive design: Test the Tailwind CSS responsive breakpoints across mobile, tablet, and desktop.<br>- Verify multilingual rendering: Ensure Sinhala text renders correctly alongside English content.<br>- Test user workflows: Complete end-to-end testing of legal research workflows (login → search → chat → view document → get recommendations → access history).                                                                                                                                                                                                                                                                         |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Technique              | - Create Playwright test suites for each major interface in src/app/\_components/ (ChatInterface.jsx, ChatBox.jsx, AppsideBarNew.jsx, etc.).<br>- Test chat functionality: Type legal query → verify AI response appears → check chat history saves.<br>- Test document search: Enter search term → verify relevant documents appear → click document → verify document loads.<br>- Test navigation: Click sidebar menu items → verify correct page loads → verify AppsideBarNew highlights active route.<br>- Test authentication flow: Register → Login → Access protected routes → Logout.<br>- Simulate Sinhala text input and verify proper rendering in chat and search interfaces.<br>- Use Cypress component testing for individual React components with various props and states.<br>- Test error states: Network failures, API timeouts, invalid inputs. |
| Oracles                | - Chat interface loads within 2 seconds and displays typing indicators during AI processing.<br>- Document search results appear within 3 seconds with relevance scores visible.<br>- History interface shows all previous chat sessions with correct timestamps.<br>- Recommendations interface displays personalized suggestions based on mock user history.<br>- Sinhala text displays correctly without broken characters or layout issues.<br>- Mobile navigation menu (hamburger menu) opens/closes correctly on smaller screens.<br>- Authentication redirects unauthorized users to login page.<br>- Lighthouse accessibility score > 90, performance score > 85.                                                                                                                                                                                           |
| Required Tools         | - **Playwright** for cross-browser end-to-end testing (Chrome, Firefox, Safari, Edge)<br>- **Cypress** for component testing and UI interaction testing<br>- **Lighthouse** for accessibility and performance audits<br>- **React Testing Library** with Jest for component unit tests<br>- **Percy** or **Chromatic** for visual regression testing<br>- **Mobile device emulators** for responsive testing (iOS Safari, Chrome Mobile)                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Success Criteria       | - All major user workflows complete successfully: registration, login, legal query, document search, chat interaction, recommendations access, history viewing, logout.<br>- Responsive design works correctly on mobile (320px-768px), tablet (768px-1024px), and desktop (1024px+).<br>- All interactive elements (buttons, inputs, links) respond within 200ms.<br>- Sinhala and English text render correctly without layout breaks.<br>- No console errors or warnings during normal operation.<br>- Accessibility audit passes with WCAG 2.1 AA compliance.                                                                                                                                                                                                                                                                                                   |
| Special Considerations | - Test Radix UI components (Dialog, DropdownMenu, Tooltip, etc.) from the shadcn/ui library used in the frontend.<br>- Verify NextAuth.js authentication flow works correctly with Supabase backend.<br>- Test AuthContext state management for user session persistence.<br>- Visual testing needed for Sinhala font rendering and RTL text handling.<br>- Test offline behavior and error boundaries for graceful degradation.<br>- Verify that legal document PDFs load correctly in the browser viewer.                                                                                                                                                                                                                                                                                                                                                         |

## 3.1.4 Performance Profiling

Performance profiling measures response times, throughput, and resource usage for LegalAI's core operations: AI chat responses, document search, vector similarity matching, and recommendation generation. The goal is to ensure the system meets performance requirements for legal professionals accessing time-sensitive information.

| Technique Objective    | Exercise core system functions (chat responses, document retrieval, FAISS vector search, recommendation generation) under normal and peak loads to measure and evaluate performance behaviors including response times, CPU/memory usage, and database query performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Technique              | · Use automated test scripts to simulate user interactions: legal queries, document searches, and recommendation requests.<br>· Measure API response times for FastAPI endpoints and Next.js page loads.<br>· Profile FAISS vector search performance with different document corpus sizes.<br>· Monitor database query performance for user authentication and chat history retrieval.<br>· Test AI model inference times for Google Gemini API calls using LangSmith tracing to capture token usage, latency, and chain execution details.<br>· Use LangSmith's monitoring dashboard to track LLM call performance, identify bottlenecks in the RAG pipeline, and analyze prompt-response patterns.<br>· Execute tests on different hardware configurations to establish performance baselines. |
| Oracles                | Automated performance monitoring tools to capture response times, throughput metrics, and resource utilization. Custom scripts to validate that chat responses return within 5 seconds, document searches within 2 seconds, and recommendations within 3 seconds under normal load.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Required Tools         | - LangSmith for LLM observability, tracing LangChain/LangGraph operations, and monitoring AI model performance<br>- Application performance monitoring tools (New Relic, Datadog, or custom Python profiling)<br>- Load testing tools (Locust, JMeter) for simulating concurrent users<br>- Database profiling tools (pg_stat_statements for PostgreSQL)<br>- Python profiling libraries (cProfile, line_profiler)<br>- Browser performance tools (Lighthouse, WebPageTest)<br>- FAISS performance benchmarking utilities                                                                                                                                                                                                                                                                     |
| Success Criteria       | Chat responses return within 5 seconds, document searches complete within 2 seconds, recommendations generate within 3 seconds, and the system handles 100 concurrent users with acceptable performance degradation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Special Considerations | - Performance testing should account for legal document processing (PDF text extraction, OCR for scanned documents)<br>- Test with realistic Sri Lankan legal corpus (acts, bills, gazettes in English and Sinhala)<br>- Monitor API rate limits for Google Gemini and Supabase services<br>- Use LangSmith to track token consumption costs and optimize prompt engineering for reduced latency<br>- Analyze LangSmith traces to identify slow components in the RAG retrieval chain (embedding generation, vector search, context assembly)<br>- Consider network latency for users in Sri Lanka accessing cloud services<br>- Profile memory usage for large document embeddings and FAISS indices                                                                                             |

---

## 3.1.5 Load Testing

Load testing subjects LegalAI to varying workloads to evaluate performance under different user loads, from individual legal professionals to peak usage during legal research periods. The goal is to ensure the system maintains acceptable performance and availability under stress.

| Technique Objective    | Exercise core transactions (legal queries, document searches, chat interactions, recommendation requests) under increasing workload conditions to observe system performance, resource utilization, and failure points.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Technique              | · Start with baseline testing: single user performing typical legal research workflows (query → search → chat → recommendations).<br>· Gradually increase concurrent users from 10 to 100+ using automated test scripts.<br>· Test different load patterns: steady state, ramp-up, spike loads, and sustained peak loads.<br>· Focus on critical paths: AI chat responses, FAISS vector searches, and database queries for legal documents.<br>· Monitor system resources (CPU, memory, network) and third-party API limits (Google Gemini, Supabase).<br>· Use LangSmith to trace and monitor LLM chains under load, capturing latency distributions, error rates, and token throughput across concurrent requests.<br>· Analyze LangSmith's run aggregation to identify performance degradation patterns as load increases.<br>· Execute tests with different data volumes (small test corpus vs. full Sri Lankan legal database). |
| Oracles                | Automated monitoring scripts to track response times, error rates, and resource usage. Load testing tools provide real-time metrics and failure detection. Success criteria include maintaining sub-5 second response times for 95% of requests under normal load, and graceful degradation under extreme load.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Required Tools         | - LangSmith for tracing LLM behavior under load, monitoring concurrent chain executions, and analyzing performance bottlenecks in AI workflows<br>- Load testing frameworks (Locust for Python-based distributed testing, JMeter for comprehensive load scenarios)<br>- Application monitoring (Prometheus, Grafana for metrics visualization)<br>- Database load monitoring (Supabase dashboard, PostgreSQL monitoring tools)<br>- API rate limiting monitors for Google Gemini and external services<br>- Cloud infrastructure monitoring if deployed on cloud platforms                                                                                                                                                                                                                                                                                                                                                       |
| Success Criteria       | The system maintains acceptable performance (response times <5s for chat, <2s for search) under 100 concurrent users, handles 1000+ legal document searches per minute, and provides graceful degradation with clear error messages during overload conditions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Special Considerations | - Load testing should simulate realistic legal research patterns (complex queries, document downloads, chat conversations)<br>- Account for geographic distribution (Sri Lankan users with varying network conditions)<br>- Test with actual legal document corpus sizes (potentially 10,000+ documents)<br>- Use LangSmith's dataset feature to create reproducible test queries and track performance regression across test runs<br>- Monitor LangSmith traces to detect rate limiting or throttling from Google Gemini API under high load<br>- Analyze LangSmith's cost tracking to project API expenses at different usage scales<br>- Monitor third-party service limits and costs during load testing<br>- Include database connection pooling and API rate limit testing<br>- Consider both horizontal scaling (multiple server instances) and vertical scaling scenarios                                                                                                                         |


#### 3.1.6 Security and Access Control Testing

Security testing for Legal AI focuses on protecting sensitive legal information, user credentials, and ensuring compliance with Sri Lankan data protection regulations. We test the JWT-based authentication system, Supabase PostgreSQL security, API endpoint protection, and prevention of common vulnerabilities (SQL injection, XSS, CSRF). For this we use **OWASP ZAP** for automated vulnerability scanning and **pytest** with security-focused test cases.

| Technique Objective    | - **Authentication Security**: Verify JWT token generation, validation, expiry, and refresh mechanisms work correctly with Supabase auth.<br>- **Authorization Testing**: Ensure users can only access their own chat history, search queries, and personalized recommendations.<br>- **API Security**: Test FastAPI endpoint protection, rate limiting, and input validation for legal queries.<br>- **Data Protection**: Verify encryption of sensitive data (passwords with bcrypt, legal queries in transit via HTTPS).<br>- **Vulnerability Assessment**: Check for SQL injection, XSS, CSRF, and other OWASP Top 10 vulnerabilities.                                                                                                                                                                                                                                                                                                                                                                                                        |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Technique              | - Test authentication endpoints: POST /auth/register, /auth/login, /auth/reset-password with valid and invalid credentials.<br>- Verify JWT tokens: Extract token from login response → use in authenticated requests → verify expiry → test refresh mechanism.<br>- Test authorization: User A attempts to access User B's chat history → should be denied with 403 Forbidden.<br>- SQL injection testing: Submit malicious SQL in legal query inputs → verify backend sanitizes input and prevents execution.<br>- XSS testing: Submit JavaScript payloads in chat messages → verify output is escaped and not executed.<br>- CSRF testing: Submit requests without CSRF tokens → verify requests are rejected.<br>- Test password security: Verify bcrypt hashing, minimum password complexity, account lockout after failed attempts.<br>- API rate limiting: Send 100+ requests rapidly → verify rate limiter activates and returns 429 Too Many Requests.<br>- Test HTTPS enforcement: Attempt HTTP connections → verify redirect to HTTPS. |
| Oracles                | - Invalid login attempts return 401 Unauthorized with appropriate error message.<br>- Expired JWT tokens are rejected with 401 and prompt for re-authentication.<br>- Unauthorized access attempts are logged in Supabase auth logs.<br>- SQL injection attempts do not execute queries or leak database information.<br>- XSS payloads are rendered as plain text, not executed as scripts.<br>- Rate limiter activates after configured threshold (e.g., 100 requests/minute).<br>- All sensitive endpoints require valid JWT authentication.<br>- Password reset emails contain secure, time-limited tokens.                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Required Tools         | - **OWASP ZAP** for automated vulnerability scanning and penetration testing<br>- **Burp Suite** for manual security testing and API request manipulation<br>- **pytest** with security test cases for backend authentication and authorization<br>- **JWT.io** for JWT token inspection and validation<br>- **Supabase Dashboard** for monitoring auth logs and security events<br>- **Postman** for API security testing with various payloads<br>- **sqlmap** for SQL injection testing (use cautiously in non-production)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Success Criteria       | - Zero critical or high-severity vulnerabilities in OWASP ZAP scan.<br>- All authentication and authorization tests pass with correct access control.<br>- Passwords are hashed with bcrypt and never stored in plain text.<br>- JWT tokens expire correctly and are validated on every request.<br>- Rate limiting prevents abuse with appropriate 429 responses.<br>- SQL injection and XSS attacks are successfully blocked.<br>- HTTPS is enforced for all connections.<br>- Security audit logs capture all authentication attempts and access violations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Special Considerations | - **Legal liability**: Unauthorized access to legal queries or user data could result in legal consequences under Sri Lankan data protection laws.<br>- **Supabase security**: Verify Row Level Security (RLS) policies in Supabase prevent cross-user data access.<br>- **Google Gemini API keys**: Ensure API keys are stored securely in environment variables, not in code.<br>- **Email security**: Test SMTP authentication and prevent email header injection in password reset flows.<br>- Coordinate with compliance team for GDPR and local data protection law adherence.<br>- Regular security audits and penetration testing by third-party security firms.                                                                                                                                                                                                                                                                                                                                                                          |

#### 3.1.7 Failover and Recovery Testing

Failover and recovery testing for Legal AI ensures the system maintains availability for critical legal information access during component failures. We test Supabase database resilience, FastAPI backend recovery, Google Gemini API fallback mechanisms, and Next.js frontend error handling. The goal is to achieve 99.9% uptime with graceful degradation during service disruptions.

| Technique Objective    | - Simulate and recover from Supabase database connection failures and verify chat history data integrity.<br>- Test FastAPI backend server failures and automatic restart mechanisms.<br>- Verify Google Gemini API timeout handling and fallback to cached responses.<br>- Test frontend error boundaries and offline functionality.<br>- Validate data backup and restore procedures for legal document corpus.<br>- Ensure user sessions persist through temporary service interruptions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ---------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Technique              | - **Database failure simulation**: Terminate Supabase connection during active chat session → verify backend error handling → restore connection → verify chat history is intact.<br>- **Backend service failure**: Kill FastAPI server process during legal query processing → verify frontend displays "Service temporarily unavailable" message → restart server → verify system recovers and queued requests are processed.<br>- **Google Gemini API failure**: Mock API timeout or 503 error → verify backend returns cached response or "AI service temporarily unavailable" message → verify user can retry query.<br>- **Network interruption**: Disconnect client network during document search → verify frontend shows offline indicator → reconnect → verify search automatically retries.<br>- **Data corruption test**: Introduce corrupted PDF in document corpus → verify OCR processing handles error gracefully → verify corrupted document is flagged for manual review.<br>- **Session persistence**: Terminate backend during active user session → verify JWT token remains valid → restart backend → verify user remains authenticated.<br>- **Database backup/restore**: Perform full Supabase backup → simulate data loss → restore from backup → verify all user data, chat histories, and document metadata are recovered. |
| Oracles                | - After database reconnection, all active user sessions continue without data loss.<br>- Backend service restarts within 30 seconds and resumes normal operation.<br>- Google Gemini API failures result in cached responses or clear error messages, not application crashes.<br>- Frontend error boundaries catch errors and display user-friendly messages instead of blank screens.<br>- Database restore completes within 15 minutes and all data integrity checks pass.<br>- User chat histories contain all messages before failure, with clear indication of temporary unavailability period.<br>- System logs capture all failure events with timestamps for incident analysis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Required Tools         | - **Supabase Dashboard** for database monitoring and backup/restore operations<br>- **systemctl** or **pm2** for managing FastAPI backend process lifecycle<br>- **Chaos Monkey** or **Gremlin** for controlled failure injection in staging environment<br>- **pytest** with failure simulation utilities for backend resilience testing<br>- **React Error Boundaries** for frontend error handling verification<br>- **Database backup tools**: pg_dump for PostgreSQL backup, Supabase automated backups<br>- **Monitoring tools**: Prometheus, Grafana for real-time system health monitoring                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Success Criteria       | - System recovers from database failures within 30 seconds with no data loss.<br>- Backend service failures are detected and restarted automatically within 60 seconds.<br>- Google Gemini API failures result in graceful degradation, not application crashes.<br>- Frontend displays appropriate error messages and recovery options during failures.<br>- Database backup and restore completes successfully with 100% data integrity.<br>- 99.9% uptime achieved through redundancy and automated recovery mechanisms.<br>- User sessions persist through temporary service interruptions (<5 minutes).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Special Considerations | - **Legal information criticality**: Legal researchers may require immediate access to legal documents for court deadlines, so minimize downtime.<br>- **Supabase managed service**: Rely on Supabase's built-in redundancy and failover capabilities, test integration with these features.<br>- **Google Gemini API limits**: Implement exponential backoff and request queuing to handle rate limit errors gracefully.<br>- **FAISS vector index recovery**: Store FAISS indices in persistent storage and implement reload mechanisms after backend restart.<br>- **Chat history durability**: Ensure Supabase transactions commit chat messages atomically to prevent partial message loss.<br>- **Monitoring and alerting**: Set up Supabase alerts for database connection issues, backend health check failures, and API error rate spikes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |

#### 3.1.8 Configuration Testing

Configuration testing for Legal AI ensures the system operates correctly across different deployment environments (development, staging, production), browser configurations, and server specifications. We test environment variable configurations, database connection settings, API key management, and frontend build configurations for Next.js and FastAPI deployments.

| Technique Objective    | - Verify the system works correctly in development (localhost), staging (cloud preview), and production (live deployment) environments.<br>- Test different browser configurations and versions across desktop and mobile platforms.<br>- Validate environment variable management for API keys, database URLs, and service configurations.<br>- Ensure Next.js build configurations (Turbopack, production builds) work correctly.<br>- Test FastAPI deployment configurations (Uvicorn workers, CORS settings, rate limiting).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Technique              | - **Environment configuration testing**:<br> - Deploy backend to development (local), staging (test server), production (cloud) and verify all environment variables load correctly (.env files).<br> - Test database connection strings for Supabase in each environment (dev, staging, prod databases).<br> - Verify Google Gemini API keys work in all environments with appropriate rate limits.<br> - Test SMTP email configuration for Gmail in each environment.<br>- **Browser configuration testing**:<br> - Test frontend on Chrome 90+, Firefox 88+, Safari 14+, Edge 90+ with default settings.<br> - Test with browser extensions installed (ad blockers, privacy tools) → verify functionality.<br> - Test with JavaScript disabled → verify graceful degradation messages.<br> - Test with different viewport sizes: Mobile (320px-768px), Tablet (768px-1024px), Desktop (1024px+).<br>- **Backend configuration testing**:<br> - Test FastAPI with 1, 4, and 8 Uvicorn workers → verify concurrent request handling.<br> - Test with different CORS origins (localhost, staging domain, production domain).<br> - Test with different rate limiting configurations (10, 50, 100 requests/minute).<br>- **Frontend build configuration testing**:<br> - Test Next.js development build (npm run dev with Turbopack) → verify hot reload works.<br> - Test Next.js production build (npm run build && npm start) → verify optimizations applied.<br> - Verify environment variables (NEXT_PUBLIC_API_URL) are correctly embedded in production build.<br>- **Database configuration testing**:<br> - Test with different Supabase connection pool sizes (5, 10, 20 connections).<br> - Test with different database query timeout settings (5s, 10s, 30s).<br>- **Resource constraint testing**:<br> - Test backend with limited memory (1GB, 2GB, 4GB) → verify FAISS index loading and query performance.<br> - Test frontend with limited bandwidth (3G, 4G, Broadband) → verify page load times. |
| Oracles                | - System functions identically across development, staging, and production environments.<br>- All environment variables load correctly and API keys authenticate successfully.<br>- Frontend renders correctly on all supported browsers without layout issues.<br>- Backend handles concurrent requests correctly with configured worker counts.<br>- CORS settings allow requests from configured origins and block others.<br>- Rate limiting activates at configured thresholds without false positives.<br>- Production builds are optimized with code splitting and lazy loading.<br>- Database connection pooling handles concurrent queries without timeout errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Required Tools         | - **Docker** for containerized environment testing and configuration isolation<br>- **dotenv** for environment variable management and validation<br>- **BrowserStack** or **Sauce Labs** for cross-browser testing automation<br>- **Lighthouse** for configuration-specific performance testing<br>- **k6** or **Artillery** for testing under different resource configurations<br>- **Supabase CLI** for database configuration testing<br>- **Python virtual environments** (venv) for backend dependency isolation<br>- **Node.js version manager** (nvm) for testing different Node versions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Success Criteria       | - System deploys successfully to all three environments (dev, staging, prod) with correct configurations.<br>- All environment variables are validated and loaded without errors on startup.<br>- Frontend works correctly on all supported browsers and viewport sizes.<br>- Backend handles expected load with configured Uvicorn workers.<br>- CORS and rate limiting configurations work as intended.<br>- Production builds pass Lighthouse performance audits (score > 85).<br>- Database connection pooling prevents "too many connections" errors under load.<br>- System gracefully handles configuration errors with clear error messages.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Special Considerations | - **Environment-specific configurations**: Development uses localhost URLs, staging uses test domains, production uses live domains - verify all configurations are correct.<br>- **API key security**: Ensure API keys are never committed to Git, always loaded from environment variables.<br>- **Next.js build optimization**: Verify production builds enable code splitting, tree shaking, and minification.<br>- **Supabase environment separation**: Use separate Supabase projects for dev, staging, and prod to prevent data contamination.<br>- **FAISS index configuration**: Large FAISS indices may require different memory configurations in each environment.<br>- **SMTP configuration**: Use different Gmail accounts or services for dev/staging/prod to prevent email quota issues.<br>- **Logging configuration**: Enable verbose logging in dev, error-only logging in production.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |

## 4. Deliverables

In this section, we'll highlight the test deliverables of LegalAI. After performing all the tests, we record the results of them. And this section includes,

- Test logs and execution reports
- Functional test reports (backend API tests, frontend component tests)
- User interface test details and screenshots
- Database integrity and performance test reports
- AI model accuracy and response quality assessments
- Security testing reports and penetration test results
- Performance profiling reports (response times, throughput metrics)
- Load testing reports (concurrent user capacity, failure points)
- Code coverage reports for automated tests
- Bug tracking and defect reports
- Test environment configuration documentation

### 4.1 Test Evaluation Summaries

Test evaluation summaries provide an idea of the status of the tests performed on Legal AI. And it reflects the successes and failures of the system after making changes to the system. This is performed after each development iteration to ensure the system works as expected even after the changes are made.

### **Backend Test Logs**

### **User Interface Test Details**

### 4.2 Reporting on Test Coverage

Test coverage reporting provides quantitative and qualitative measures of how thoroughly the LegalAI system has been tested. Coverage metrics ensure that all critical functionality, user workflows, and system components are adequately validated.

**Coverage Areas:**

- **Code Coverage**: Target 80%+ coverage for Python backend (FastAPI routes, services, utilities) and 70%+ for React frontend components
- **API Endpoint Coverage**: All REST API endpoints tested with positive/negative scenarios
- **User Workflow Coverage**: Complete coverage of legal research workflows (query → search → chat → recommendations)
- **Data Coverage**: Testing with diverse legal document types (acts, bills, gazettes in English/Sinhala)
- **Browser/Device Coverage**: Testing across Chrome, Firefox, Safari, Edge on desktop/mobile
- **Error Scenario Coverage**: Invalid inputs, network failures, API timeouts, database connection issues
- **Security Coverage**: Authentication, authorization, data privacy, and input validation
- **Performance Coverage**: Response times, concurrent users, resource utilization under various loads

**Coverage Metrics:**

- Statement coverage, branch coverage, and path coverage for critical algorithms
- User interface element coverage (all interactive components tested)
- Database operation coverage (CRUD operations for all entities)
- Third-party integration coverage (Supabase, Google Gemini, SMTP services)
- Edge case coverage for legal document processing (OCR failures, corrupted PDFs, large documents)

**Reporting Format:**

- Coverage reports generated by pytest-cov, Jest, and Cypress
- Heat maps showing tested vs. untested code paths
- Gap analysis reports identifying untested functionality
- Recommendations for additional test cases to achieve target coverage

## 5. Risks, Dependencies, Assumptions, and Constraints

| Risk                                    | Mitigation Strategy                                                                                       | Contingency (Risk is realized)                                          |
| --------------------------------------- | --------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| Prerequisite entry criteria is not met. | Define clear entry criteria including database setup, API keys, and test data availability.               | Meet outstanding prerequisites or delay testing until ready.            |
| Test data proves to be inadequate.      | Prepare comprehensive test datasets including Sri Lankan legal documents, user scenarios, and edge cases. | Generate synthetic test data or use anonymized production data samples. |
| Database requires refresh.              | <System Admin> will endeavor to ensure the Database is regularly refreshed as required by <Tester>.       | - Restore data and restart<br>- Clear Database                          |

## 6. References

- Indicate the tool references (name, available at &lt;<URL&gt;>) used for testing
- Refer any data/ information in a standard format (eg. IEEE referencing style)
- For different algorithms/ techniques/ theories you can refer text books.
- For tools you can refer web pages.
- For similar work you can refer research paper articles that describe the work.
- You may include white paper articles for the description of technologies; web URL for the tool references. When you refer such a web page, you have to indicate the (Accessed on &lt;<date&gt;>)
