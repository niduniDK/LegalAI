{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78fdef2",
   "metadata": {},
   "source": [
    "# Sri Lankan Legal AI Dataset for Unsloth Gemma 3\n",
    "\n",
    "Simple dataset preparation for fine-tuning Gemma 3 with Sri Lankan legal data.\n",
    "\n",
    "## Dataset Sources:\n",
    "1. `finetune_data.json` - Constitutional Law data\n",
    "2. `finetune_penalCode.json` - Penal Code data  \n",
    "3. `finetuneData_penalcode2.json` - Additional Penal Code data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ad5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install datasets huggingface_hub pandas python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f02a832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hugging Face token loaded from .env file\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import HfApi\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "if HF_TOKEN:\n",
    "    print(\"‚úÖ Hugging Face token loaded from .env file\")\n",
    "else:\n",
    "    print(\"‚ùå No HF_TOKEN found in .env file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca6bfe",
   "metadata": {},
   "source": [
    "## Load JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2645cc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 601 entries from docs/finetune_data.json\n",
      "‚úÖ Loaded 890 entries from docs/finetune_penalCode.json\n",
      "‚úÖ Loaded 133 entries from docs/finetuneData_penalcode2.json\n",
      "\n",
      "üìä Dataset Summary:\n",
      "Constitutional Law: 601 entries\n",
      "Penal Code 1: 890 entries\n",
      "Penal Code 2: 133 entries\n",
      "Total: 1624 entries\n"
     ]
    }
   ],
   "source": [
    "def load_json_file(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load JSON file and return data\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"‚úÖ Loaded {len(data)} entries from {file_path}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load all dataset files\n",
    "constitutional_data = load_json_file('docs/finetune_data.json')\n",
    "penal_code_data1 = load_json_file('docs/finetune_penalCode.json')\n",
    "penal_code_data2 = load_json_file('docs/finetuneData_penalcode2.json')\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"Constitutional Law: {len(constitutional_data)} entries\")\n",
    "print(f\"Penal Code 1: {len(penal_code_data1)} entries\")\n",
    "print(f\"Penal Code 2: {len(penal_code_data2)} entries\")\n",
    "print(f\"Total: {len(constitutional_data) + len(penal_code_data1) + len(penal_code_data2)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273ecd9",
   "metadata": {},
   "source": [
    "## Convert to Conversations Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504a14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction_from_content(content: str) -> str:\n",
    "    \"\"\"Generate appropriate instruction based on content\"\"\"\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    if 'freedom of speech' in content_lower:\n",
    "        return \"Explain the legal framework of freedom of speech in Sri Lankan law\"\n",
    "    elif any(word in content_lower for word in ['murder', 'homicide']):\n",
    "        return \"Define and explain homicide and murder under Sri Lankan Penal Code\"\n",
    "    elif 'theft' in content_lower:\n",
    "        return \"Explain the legal definition and punishment for theft\"\n",
    "    elif 'public servant' in content_lower:\n",
    "        return \"Define public servant according to Sri Lankan Penal Code\"\n",
    "    elif any(word in content_lower for word in ['constitution', 'constitutional']):\n",
    "        return \"Explain this constitutional law concept\"\n",
    "    else:\n",
    "        return \"Explain this legal concept under Sri Lankan law\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f37e3a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted:\n",
      "Constitutional: 601 conversations\n",
      "Penal Code 1: 890 conversations\n",
      "Penal Code 2: 133 conversations\n"
     ]
    }
   ],
   "source": [
    "def convert_to_conversations(data: List[Dict], category: str) -> List[Dict]:\n",
    "    \"\"\"Convert data to conversations format for Unsloth\"\"\"\n",
    "    conversations = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Get user message\n",
    "        if 'input' in item:\n",
    "            user_message = item['input']\n",
    "        else:\n",
    "            user_message = generate_instruction_from_content(item.get('output', ''))\n",
    "        \n",
    "        # Get assistant message\n",
    "        if 'output' in item:\n",
    "            if 'legalDoc' in item and item['legalDoc'].strip():\n",
    "                assistant_message = f\"According to {item['legalDoc']}: {item['output']}\"\n",
    "            else:\n",
    "                assistant_message = item['output']\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Create conversation\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "        ]\n",
    "        \n",
    "        conversations.append({\n",
    "            'conversations': conversation,\n",
    "            'category': category\n",
    "        })\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Convert all data\n",
    "constitutional_conversations = convert_to_conversations(constitutional_data, 'constitutional_law')\n",
    "penal_conversations1 = convert_to_conversations(penal_code_data1, 'penal_code')\n",
    "penal_conversations2 = convert_to_conversations(penal_code_data2, 'penal_code')\n",
    "\n",
    "print(f\"Converted:\")\n",
    "print(f\"Constitutional: {len(constitutional_conversations)} conversations\")\n",
    "print(f\"Penal Code 1: {len(penal_conversations1)} conversations\")\n",
    "print(f\"Penal Code 2: {len(penal_conversations2)} conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3dadb5",
   "metadata": {},
   "source": [
    "## Combine and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730b991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset created:\n",
      "Total: 1624 conversations\n",
      "Training: 1299 conversations\n",
      "Validation: 325 conversations\n",
      "\n",
      "üîç Sample conversation:\n",
      "User: What is the punishment for importing counterfeit coin?...\n",
      "Assistant: According to Penal Code of Sri Lanka, Section 234 - Import or export of machine or instrument for co...\n"
     ]
    }
   ],
   "source": [
    "# Combine all data\n",
    "all_conversations = constitutional_conversations + penal_conversations1 + penal_conversations2\n",
    "\n",
    "# Shuffle\n",
    "random.seed(42)\n",
    "random.shuffle(all_conversations)\n",
    "\n",
    "# Split 80/20\n",
    "split_idx = int(0.8 * len(all_conversations))\n",
    "train_data = all_conversations[:split_idx]\n",
    "val_data = all_conversations[split_idx:]\n",
    "\n",
    "print(f\"üìä Dataset created:\")\n",
    "print(f\"Total: {len(all_conversations)} conversations\")\n",
    "print(f\"Training: {len(train_data)} conversations\")\n",
    "print(f\"Validation: {len(val_data)} conversations\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nüîç Sample conversation:\")\n",
    "sample = train_data[0]\n",
    "print(f\"User: {sample['conversations'][0]['content'][:100]}...\")\n",
    "print(f\"Assistant: {sample['conversations'][1]['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a10264",
   "metadata": {},
   "source": [
    "## Create Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96666c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hugging Face dataset created!\n",
      "Features: {'conversations': List({'content': Value('string'), 'role': Value('string')}), 'category': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Hugging Face dataset created!\")\n",
    "print(f\"Features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53df466",
   "metadata": {},
   "source": [
    "## Upload to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7bd6081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Uploading to Nishan726/sri-lankan-legal-conversations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b9d0fffefd405a9e47cde48a9c1ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd0969d4666488fb1a0ad7d5a531bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337a092616824440bbf3dc2b10e418bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7d45e8d9d347c1b7447425d2a53a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Upload successful!\n",
      "üîó Dataset: https://huggingface.co/datasets/Nishan726/sri-lankan-legal-conversations\n",
      "\n",
      "üìã Use in Gemma 3 notebook:\n",
      "dataset = load_dataset(\"Nishan726/sri-lankan-legal-conversations\", split=\"train\")\n"
     ]
    }
   ],
   "source": [
    "# Upload dataset\n",
    "DATASET_NAME = \"Nishan726/sri-lankan-legal-conversations\"\n",
    "\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        print(f\"üöÄ Uploading to {DATASET_NAME}...\")\n",
    "        dataset_dict.push_to_hub(DATASET_NAME, token=HF_TOKEN, private=False)\n",
    "        print(f\"‚úÖ Upload successful!\")\n",
    "        print(f\"üîó Dataset: https://huggingface.co/datasets/{DATASET_NAME}\")\n",
    "        print(f\"\\nüìã Use in Gemma 3 notebook:\")\n",
    "        print(f'dataset = load_dataset(\"{DATASET_NAME}\", split=\"train\")')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No HF_TOKEN found. Add it to .env file first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcfae13",
   "metadata": {},
   "source": [
    "## Save Locally (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccee184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Dataset saved locally as sri_lankan_legal_dataset.json\n",
      "\n",
      "üéØ Ready for Gemma 3 fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "# Save datasets locally\n",
    "with open('sri_lankan_legal_dataset.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'train': train_data,\n",
    "        'validation': val_data,\n",
    "        'info': {\n",
    "            'total': len(all_conversations),\n",
    "            'train': len(train_data),\n",
    "            'validation': len(val_data)\n",
    "        }\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"üíæ Dataset saved locally as sri_lankan_legal_dataset.json\")\n",
    "print(\"\\nüéØ Ready for Gemma 3 fine-tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a671cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
