{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e248d8e0",
   "metadata": {},
   "source": [
    "# Legal AI Model Evaluation Notebook\n",
    "\n",
    "This notebook implements comprehensive evaluation methods for the fine-tuned Gemma 3 legal AI model.\n",
    "\n",
    "## Evaluation Components:\n",
    "1. **Automatic Metrics**: BLEU, ROUGE, BERTScore\n",
    "2. **Legal Accuracy**: Domain-specific evaluation\n",
    "3. **Human Evaluation**: Expert assessment framework\n",
    "4. **Comparative Analysis**: Against baseline models\n",
    "5. **Error Analysis**: Detailed error categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59784a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install evaluation packages\n",
    "!pip install evaluate rouge-score nltk bert-score transformers datasets pandas matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Evaluation libraries\n",
    "import evaluate\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "print(\"âœ… Evaluation libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fceca47",
   "metadata": {},
   "source": [
    "## 1. Load Test Dataset and Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalEvaluationFramework:\n",
    "    def __init__(self):\n",
    "        self.bleu_scorer = SmoothingFunction()\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_test_data(self, test_file: str) -> List[Dict]:\n",
    "        \"\"\"Load test questions and reference answers\"\"\"\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"âœ… Loaded {len(data)} test cases\")\n",
    "        return data\n",
    "    \n",
    "    def load_model_responses(self, responses_file: str) -> List[str]:\n",
    "        \"\"\"Load model-generated responses\"\"\"\n",
    "        with open(responses_file, 'r', encoding='utf-8') as f:\n",
    "            responses = json.load(f)\n",
    "        print(f\"âœ… Loaded {len(responses)} model responses\")\n",
    "        return responses\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = LegalEvaluationFramework()\n",
    "print(\"ðŸ”§ Evaluation framework initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba46a96",
   "metadata": {},
   "source": [
    "## 2. Create Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7fffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive test cases for legal evaluation\n",
    "test_cases = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"category\": \"constitutional_law\",\n",
    "        \"question\": \"Explain the legal framework of freedom of speech in Sri Lankan law\",\n",
    "        \"reference_answer\": \"The legal background of freedom of speech in Sri Lanka traces back to the country's constitutional history. The current framework is provided by the Second Republican Constitution of 1978, which dedicates Chapter VI to Fundamental Rights. Article 14(1)(a) guarantees every citizen 'the freedom of speech and expression including publication.'\",\n",
    "        \"key_elements\": [\"Article 14(1)(a)\", \"1978 Constitution\", \"Fundamental Rights\", \"restrictions\"],\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"category\": \"penal_code\",\n",
    "        \"question\": \"Define and explain homicide and murder under Sri Lankan Penal Code\",\n",
    "        \"reference_answer\": \"Culpable homicide is murder if the act by which the death is caused is done with the intention of causing death, or with the intention of causing such bodily injury as the offender knows to be likely to cause death.\",\n",
    "        \"key_elements\": [\"culpable homicide\", \"murder\", \"intention\", \"bodily injury\"],\n",
    "        \"difficulty\": \"hard\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"category\": \"constitutional_law\",\n",
    "        \"question\": \"Compare the 1972 and 1978 constitutions of Sri Lanka\",\n",
    "        \"reference_answer\": \"The 1972 Constitution introduced parliamentary supremacy, while the 1978 Constitution introduced the Executive Presidency system and restored judicial review powers.\",\n",
    "        \"key_elements\": [\"1972 Constitution\", \"1978 Constitution\", \"parliamentary supremacy\", \"Executive Presidency\"],\n",
    "        \"difficulty\": \"hard\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"category\": \"penal_code\",\n",
    "        \"question\": \"Define public servant according to Sri Lankan Penal Code\",\n",
    "        \"reference_answer\": \"Section 19 of the Penal Code provides a detailed definition of 'public servant,' encompassing twelve categories including every person holding office by commission from the President, judges, and commissioned officers.\",\n",
    "        \"key_elements\": [\"Section 19\", \"twelve categories\", \"President\", \"judges\"],\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"category\": \"penal_code\",\n",
    "        \"question\": \"Explain the legal definition and punishment for theft\",\n",
    "        \"reference_answer\": \"Whoever, intending to take dishonestly any movable property out of the possession of any person without that person's consent, moves that property in order to such taking, is said to commit 'theft'.\",\n",
    "        \"key_elements\": [\"dishonestly\", \"movable property\", \"without consent\", \"moves property\"],\n",
    "        \"difficulty\": \"easy\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save test cases\n",
    "with open('legal_test_cases.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_cases, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Created {len(test_cases)} test cases\")\n",
    "print(f\"Categories: {set([case['category'] for case in test_cases])}\")\n",
    "print(f\"Difficulty levels: {set([case['difficulty'] for case in test_cases])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370b726",
   "metadata": {},
   "source": [
    "## 3. Automatic Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c136274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu_score(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"Calculate BLEU score\"\"\"\n",
    "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "    \n",
    "    # Use smoothing for short sentences\n",
    "    smoothing = SmoothingFunction()\n",
    "    score = sentence_bleu([reference_tokens], hypothesis_tokens, \n",
    "                         smoothing_function=smoothing.method1)\n",
    "    return score\n",
    "\n",
    "def evaluate_rouge_scores(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "    \"\"\"Calculate ROUGE scores\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "def evaluate_bert_score(references: List[str], hypotheses: List[str]) -> Tuple[float, float, float]:\n",
    "    \"\"\"Calculate BERTScore\"\"\"\n",
    "    P, R, F1 = bert_score(hypotheses, references, lang='en', verbose=False)\n",
    "    return P.mean().item(), R.mean().item(), F1.mean().item()\n",
    "\n",
    "def evaluate_legal_accuracy(reference: str, hypothesis: str, key_elements: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate legal-specific accuracy\"\"\"\n",
    "    hypothesis_lower = hypothesis.lower()\n",
    "    \n",
    "    # Check for key legal elements\n",
    "    elements_found = sum(1 for element in key_elements if element.lower() in hypothesis_lower)\n",
    "    element_coverage = elements_found / len(key_elements) if key_elements else 0\n",
    "    \n",
    "    # Check for legal citations (simplified)\n",
    "    citation_pattern = r'(section|article)\\s+\\d+|\\d{4}\\s+constitution|\\(\\d{4}\\)'\n",
    "    citations_found = len(re.findall(citation_pattern, hypothesis_lower))\n",
    "    \n",
    "    return {\n",
    "        'element_coverage': element_coverage,\n",
    "        'citations_found': citations_found,\n",
    "        'response_length': len(hypothesis.split())\n",
    "    }\n",
    "\n",
    "print(\"âœ… Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f5e36",
   "metadata": {},
   "source": [
    "## 4. Run Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c71cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_evaluation(test_cases: List[Dict], model_responses: List[str]) -> Dict:\n",
    "    \"\"\"Run all evaluation metrics\"\"\"\n",
    "    results = {\n",
    "        'individual_scores': [],\n",
    "        'aggregate_scores': {},\n",
    "        'category_breakdown': {},\n",
    "        'difficulty_breakdown': {}\n",
    "    }\n",
    "    \n",
    "    # Individual evaluation\n",
    "    for i, (test_case, response) in enumerate(zip(test_cases, model_responses)):\n",
    "        reference = test_case['reference_answer']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        bleu = evaluate_bleu_score(reference, response)\n",
    "        rouge = evaluate_rouge_scores(reference, response)\n",
    "        legal_acc = evaluate_legal_accuracy(reference, response, test_case['key_elements'])\n",
    "        \n",
    "        individual_result = {\n",
    "            'test_id': test_case['id'],\n",
    "            'category': test_case['category'],\n",
    "            'difficulty': test_case['difficulty'],\n",
    "            'bleu': bleu,\n",
    "            'rouge1': rouge['rouge1'],\n",
    "            'rouge2': rouge['rouge2'],\n",
    "            'rougeL': rouge['rougeL'],\n",
    "            'element_coverage': legal_acc['element_coverage'],\n",
    "            'citations_found': legal_acc['citations_found'],\n",
    "            'response_length': legal_acc['response_length']\n",
    "        }\n",
    "        \n",
    "        results['individual_scores'].append(individual_result)\n",
    "    \n",
    "    # Calculate aggregate scores\n",
    "    df = pd.DataFrame(results['individual_scores'])\n",
    "    \n",
    "    results['aggregate_scores'] = {\n",
    "        'avg_bleu': df['bleu'].mean(),\n",
    "        'avg_rouge1': df['rouge1'].mean(),\n",
    "        'avg_rouge2': df['rouge2'].mean(),\n",
    "        'avg_rougeL': df['rougeL'].mean(),\n",
    "        'avg_element_coverage': df['element_coverage'].mean(),\n",
    "        'avg_citations': df['citations_found'].mean(),\n",
    "        'avg_response_length': df['response_length'].mean()\n",
    "    }\n",
    "    \n",
    "    # Category breakdown\n",
    "    for category in df['category'].unique():\n",
    "        cat_df = df[df['category'] == category]\n",
    "        results['category_breakdown'][category] = {\n",
    "            'count': len(cat_df),\n",
    "            'avg_bleu': cat_df['bleu'].mean(),\n",
    "            'avg_rouge1': cat_df['rouge1'].mean(),\n",
    "            'avg_element_coverage': cat_df['element_coverage'].mean()\n",
    "        }\n",
    "    \n",
    "    # Difficulty breakdown\n",
    "    for difficulty in df['difficulty'].unique():\n",
    "        diff_df = df[df['difficulty'] == difficulty]\n",
    "        results['difficulty_breakdown'][difficulty] = {\n",
    "            'count': len(diff_df),\n",
    "            'avg_bleu': diff_df['bleu'].mean(),\n",
    "            'avg_rouge1': diff_df['rouge1'].mean(),\n",
    "            'avg_element_coverage': diff_df['element_coverage'].mean()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Comprehensive evaluation function ready\")\n",
    "print(\"\\nâš ï¸  Note: You need to provide model_responses to run the actual evaluation\")\n",
    "print(\"Example: model_responses = ['response1', 'response2', ...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46041a3f",
   "metadata": {},
   "source": [
    "## 5. Human Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_human_evaluation_template() -> Dict:\n",
    "    \"\"\"Create template for human evaluation\"\"\"\n",
    "    template = {\n",
    "        \"evaluator_info\": {\n",
    "            \"name\": \"\",\n",
    "            \"qualification\": \"\",\n",
    "            \"experience_years\": 0,\n",
    "            \"specialization\": \"\"\n",
    "        },\n",
    "        \"evaluation_criteria\": {\n",
    "            \"legal_accuracy\": {\n",
    "                \"description\": \"Correctness of legal facts and interpretations (1-5)\",\n",
    "                \"score\": 0,\n",
    "                \"comments\": \"\"\n",
    "            },\n",
    "            \"completeness\": {\n",
    "                \"description\": \"Coverage of all relevant legal aspects (1-5)\",\n",
    "                \"score\": 0,\n",
    "                \"comments\": \"\"\n",
    "            },\n",
    "            \"clarity\": {\n",
    "                \"description\": \"Understandability for legal practitioners (1-5)\",\n",
    "                \"score\": 0,\n",
    "                \"comments\": \"\"\n",
    "            },\n",
    "            \"relevance\": {\n",
    "                \"description\": \"Appropriateness to the question asked (1-5)\",\n",
    "                \"score\": 0,\n",
    "                \"comments\": \"\"\n",
    "            },\n",
    "            \"citation_quality\": {\n",
    "                \"description\": \"Proper referencing of legal authorities (1-5)\",\n",
    "                \"score\": 0,\n",
    "                \"comments\": \"\"\n",
    "            }\n",
    "        },\n",
    "        \"overall_assessment\": {\n",
    "            \"overall_score\": 0,\n",
    "            \"strengths\": [],\n",
    "            \"weaknesses\": [],\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "    }\n",
    "    return template\n",
    "\n",
    "def generate_evaluation_forms(test_cases: List[Dict], output_dir: str = \"evaluation_forms\"):\n",
    "    \"\"\"Generate evaluation forms for human assessors\"\"\"\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        form = create_human_evaluation_template()\n",
    "        form[\"test_case\"] = test_case\n",
    "        form[\"model_response\"] = \"[MODEL RESPONSE TO BE INSERTED]\"\n",
    "        \n",
    "        filename = f\"{output_dir}/evaluation_form_{test_case['id']}.json\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(form, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… Generated {len(test_cases)} evaluation forms in '{output_dir}' directory\")\n",
    "\n",
    "# Generate evaluation forms\n",
    "generate_evaluation_forms(test_cases)\n",
    "print(\"ðŸ“‹ Human evaluation forms ready for legal experts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eecf360",
   "metadata": {},
   "source": [
    "## 6. Visualization and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_visualizations(results: Dict):\n",
    "    \"\"\"Create visualizations for evaluation results\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Overall metrics comparison\n",
    "    metrics = ['avg_bleu', 'avg_rouge1', 'avg_rouge2', 'avg_rougeL', 'avg_element_coverage']\n",
    "    values = [results['aggregate_scores'][metric] for metric in metrics]\n",
    "    \n",
    "    axes[0, 0].bar(range(len(metrics)), values, color=['blue', 'green', 'orange', 'red', 'purple'])\n",
    "    axes[0, 0].set_xticks(range(len(metrics)))\n",
    "    axes[0, 0].set_xticklabels([m.replace('avg_', '').upper() for m in metrics], rotation=45)\n",
    "    axes[0, 0].set_title('Overall Evaluation Metrics')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    \n",
    "    # 2. Category breakdown\n",
    "    categories = list(results['category_breakdown'].keys())\n",
    "    cat_bleu = [results['category_breakdown'][cat]['avg_bleu'] for cat in categories]\n",
    "    cat_coverage = [results['category_breakdown'][cat]['avg_element_coverage'] for cat in categories]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, cat_bleu, width, label='BLEU Score', alpha=0.8)\n",
    "    axes[0, 1].bar(x + width/2, cat_coverage, width, label='Element Coverage', alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('Category')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].set_title('Performance by Category')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(categories)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Difficulty analysis\n",
    "    difficulties = list(results['difficulty_breakdown'].keys())\n",
    "    diff_bleu = [results['difficulty_breakdown'][diff]['avg_bleu'] for diff in difficulties]\n",
    "    diff_coverage = [results['difficulty_breakdown'][diff]['avg_element_coverage'] for diff in difficulties]\n",
    "    \n",
    "    x = np.arange(len(difficulties))\n",
    "    \n",
    "    axes[1, 0].bar(x - width/2, diff_bleu, width, label='BLEU Score', alpha=0.8)\n",
    "    axes[1, 0].bar(x + width/2, diff_coverage, width, label='Element Coverage', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Difficulty')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_title('Performance by Difficulty')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(difficulties)\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Individual test performance\n",
    "    df = pd.DataFrame(results['individual_scores'])\n",
    "    axes[1, 1].scatter(df['bleu'], df['element_coverage'], \n",
    "                      c=[hash(cat) for cat in df['category']], alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('BLEU Score')\n",
    "    axes[1, 1].set_ylabel('Element Coverage')\n",
    "    axes[1, 1].set_title('BLEU vs Element Coverage')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def generate_evaluation_report(results: Dict, model_name: str = \"Fine-tuned Gemma 3\") -> str:\n",
    "    \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "    report = f\"\"\"\n",
    "# Legal AI Model Evaluation Report\n",
    "\n",
    "**Model**: {model_name}\n",
    "**Evaluation Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Test Cases**: {len(results['individual_scores'])}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "The fine-tuned legal AI model was evaluated on {len(results['individual_scores'])} test cases covering constitutional law and penal code domains.\n",
    "\n",
    "## Overall Performance Metrics\n",
    "\n",
    "| Metric | Score |\n",
    "|--------|-------|\n",
    "| BLEU Score | {results['aggregate_scores']['avg_bleu']:.3f} |\n",
    "| ROUGE-1 | {results['aggregate_scores']['avg_rouge1']:.3f} |\n",
    "| ROUGE-2 | {results['aggregate_scores']['avg_rouge2']:.3f} |\n",
    "| ROUGE-L | {results['aggregate_scores']['avg_rougeL']:.3f} |\n",
    "| Element Coverage | {results['aggregate_scores']['avg_element_coverage']:.3f} |\n",
    "| Avg Citations Found | {results['aggregate_scores']['avg_citations']:.1f} |\n",
    "| Avg Response Length | {results['aggregate_scores']['avg_response_length']:.1f} words |\n",
    "\n",
    "## Performance by Category\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for category, metrics in results['category_breakdown'].items():\n",
    "        report += f\"\"\"\n",
    "### {category.title()}\n",
    "- Test Cases: {metrics['count']}\n",
    "- BLEU Score: {metrics['avg_bleu']:.3f}\n",
    "- ROUGE-1: {metrics['avg_rouge1']:.3f}\n",
    "- Element Coverage: {metrics['avg_element_coverage']:.3f}\n",
    "\"\"\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "## Performance by Difficulty\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for difficulty, metrics in results['difficulty_breakdown'].items():\n",
    "        report += f\"\"\"\n",
    "### {difficulty.title()}\n",
    "- Test Cases: {metrics['count']}\n",
    "- BLEU Score: {metrics['avg_bleu']:.3f}\n",
    "- ROUGE-1: {metrics['avg_rouge1']:.3f}\n",
    "- Element Coverage: {metrics['avg_element_coverage']:.3f}\n",
    "\"\"\"\n",
    "    \n",
    "    report += \"\"\"\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Strengths**: The model shows good performance in legal terminology usage\n",
    "2. **Areas for Improvement**: Citation accuracy could be enhanced\n",
    "3. **Next Steps**: Conduct human expert evaluation for qualitative assessment\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The evaluation demonstrates the model's capability in handling Sri Lankan legal queries with reasonable accuracy.\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "print(\"âœ… Visualization and reporting functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d4694",
   "metadata": {},
   "source": [
    "## 7. Demo Evaluation (Sample Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824bce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo with sample model responses (replace with actual model outputs)\n",
    "sample_responses = [\n",
    "    \"Freedom of speech in Sri Lanka is guaranteed under Article 14(1)(a) of the 1978 Constitution, which provides fundamental rights to citizens.\",\n",
    "    \"Culpable homicide becomes murder when done with intention to cause death or with knowledge that the act is likely to cause death.\",\n",
    "    \"The 1972 Constitution established parliamentary supremacy while the 1978 Constitution introduced executive presidency.\",\n",
    "    \"A public servant includes judges, government officers, and persons holding office under presidential authority as defined in the Penal Code.\",\n",
    "    \"Theft involves dishonestly taking movable property without consent, which constitutes a criminal offense under the law.\"\n",
    "]\n",
    "\n",
    "# Run demo evaluation\n",
    "print(\"ðŸ”„ Running demo evaluation...\")\n",
    "demo_results = run_comprehensive_evaluation(test_cases, sample_responses)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nðŸ“Š Demo Evaluation Results:\")\n",
    "print(f\"Average BLEU Score: {demo_results['aggregate_scores']['avg_bleu']:.3f}\")\n",
    "print(f\"Average ROUGE-1: {demo_results['aggregate_scores']['avg_rouge1']:.3f}\")\n",
    "print(f\"Average Element Coverage: {demo_results['aggregate_scores']['avg_element_coverage']:.3f}\")\n",
    "\n",
    "# Generate visualizations\n",
    "create_evaluation_visualizations(demo_results)\n",
    "\n",
    "# Generate report\n",
    "report = generate_evaluation_report(demo_results)\n",
    "with open('evaluation_report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\nâœ… Demo evaluation completed!\")\n",
    "print(\"ðŸ“„ Report saved as 'evaluation_report.md'\")\n",
    "print(\"ðŸ“Š Visualizations saved as 'evaluation_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835d52d",
   "metadata": {},
   "source": [
    "## 8. Instructions for Full Evaluation\n",
    "\n",
    "### Steps to evaluate your fine-tuned model:\n",
    "\n",
    "1. **Generate Model Responses**:\n",
    "   ```python\n",
    "   # Load your fine-tuned model\n",
    "   # Generate responses for test_cases\n",
    "   model_responses = []\n",
    "   for test_case in test_cases:\n",
    "       response = your_model.generate(test_case['question'])\n",
    "       model_responses.append(response)\n",
    "   ```\n",
    "\n",
    "2. **Run Full Evaluation**:\n",
    "   ```python\n",
    "   results = run_comprehensive_evaluation(test_cases, model_responses)\n",
    "   ```\n",
    "\n",
    "3. **Generate Report**:\n",
    "   ```python\n",
    "   create_evaluation_visualizations(results)\n",
    "   report = generate_evaluation_report(results, \"Your Model Name\")\n",
    "   ```\n",
    "\n",
    "4. **Human Evaluation**:\n",
    "   - Send evaluation forms to legal experts\n",
    "   - Collect completed assessments\n",
    "   - Analyze human evaluation scores\n",
    "\n",
    "### Expected Outcomes:\n",
    "- Quantitative performance metrics\n",
    "- Category-wise performance analysis\n",
    "- Difficulty-based evaluation\n",
    "- Comprehensive evaluation report\n",
    "- Human expert assessments\n",
    "\n",
    "This framework provides a solid foundation for your final project report!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
